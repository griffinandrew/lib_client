

// Hybrid allocator using DRAM (jemalloc) up to a limit, then PMEM (UMF)
use core::alloc::{GlobalAlloc, Layout};
use std::sync::{Once, atomic::{AtomicUsize, Ordering}};
use std::ptr;
use tikv_jemallocator::Jemalloc;
use std::alloc::{Allocator, AllocError};
use std::ptr::NonNull;

mod allocator_bindings {
    include!("umf_allocator_bindings.rs"); // generated by bindgen
}

/// Hybrid allocator: first DRAM up to a limit, then PMEM

#[derive(Clone, Copy)]
pub struct HybridObjects;

//pub mod allocator;

static INIT: Once = Once::new();
static DRAM_ALLOCATED_OBJECTS: AtomicUsize = AtomicUsize::new(0);

static mut DRAM_LIMIT_OBJECTS: usize = 0; // try with all in pmem......

static PRINT_THRESHOLD: usize = 10000;
static mut NUM_ALLOCS: usize = 0;
static mut NUM_DEALLOCS: usize = 0;
static ALL_MEM_ALLOCATED: AtomicUsize = AtomicUsize::new(0);


unsafe impl GlobalAlloc for HybridObjects {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        // Decide backend

        //if only using pmem ... dont track the fucking counters.... 
        if DRAM_LIMIT_OBJECTS == 0 {
            unsafe {
                INIT.call_once(|| {
                    let dax_size = 236757975040; // PMEM size from ndctl list --namespaces
                    let dax_path = b"/dev/dax0.0\0".as_ptr() as *const i8; // PMEM path from ndctl list --namespaces
                    allocator_bindings::umf_allocator_init(
                        dax_path,
                        dax_size,
                        );
                    #[cfg(debug_assertions)] {println!("Initialized PMEM allocator with DAX path /dev/dax0.0 and size {}", dax_size);}
                        //println!("CACHE: Initialized PMEM allocator with DAX path /dev/dax0.0 and size {}", dax_size);
                });
            }
            let ptr = allocator_bindings::umf_alloc(layout.size(), layout.align()) as *mut u8;
            if ptr.is_null() { println!("Failed to allocate PMEM"); return ptr::null_mut(); }
            //println!("CACHE: Allocated {} bytes from PMEM", layout.size());
            //println!("CACHE: allocated size (align): {}", layout.align());
            //println!("CACHE: allocated ptr: {:p}", ptr);
            return ptr;
        }

        if DRAM_LIMIT_OBJECTS == 1000 * 1024 * 1024 * 1024 {
            let ptr = Jemalloc.alloc(layout);
            if ptr.is_null() { println!("Failed to allocate DRAM"); return ptr::null_mut(); }
            return ptr;
        }

        let raw = if Self::should_use_dram(layout.size()) {
            let ptr = Jemalloc.alloc(layout);
            if ptr.is_null() { println!("Failed to allocate DRAM"); return ptr::null_mut(); }
            DRAM_ALLOCATED_OBJECTS.fetch_add(layout.size(), Ordering::SeqCst);
            #[cfg(debug_assertions)] {ALL_MEM_ALLOCATED.fetch_add(layout.size(), Ordering::SeqCst);}
            ptr
        } else {
            unsafe {
                INIT.call_once(|| {
                    let dax_size = 118377938944; // PMEM size from ndctl list --namespaces
                    let dax_path = b"/dev/dax0.0\0".as_ptr() as *const i8; // PMEM path from ndctl list --namespaces
                    allocator_bindings::umf_allocator_init(
                        dax_path,
                        dax_size,
                        );
                });
            }
            let ptr = allocator_bindings::umf_alloc(layout.size(), layout.align()) as *mut u8;
            if ptr.is_null() { println!("Failed to allocate PMEM"); return ptr::null_mut(); }
            #[cfg(debug_assertions)] {ALL_MEM_ALLOCATED.fetch_add(layout.size(), Ordering::SeqCst);}
            ptr
        };
       
        #[cfg(debug_assertions)] {
            unsafe {

                if PRINT_THRESHOLD < NUM_ALLOCS {
                    let total_alloc = ALL_MEM_ALLOCATED.load(Ordering::SeqCst);
                    let dram_alloc = DRAM_ALLOCATED_OBJECTS.load(Ordering::SeqCst);
                    println!("Allocated: total {} bytes, DRAM {} bytes ({:.2}%), PMEM {} bytes ({:.2}%)",
                        total_alloc,
                        dram_alloc,
                        dram_alloc as f64 / total_alloc as f64 * 100.0,
                        total_alloc - dram_alloc,
                        (total_alloc - dram_alloc) as f64 / total_alloc as f64 * 100.0,
                    );
                    println!("alloc requested layout size: {}", layout.size());
                    println!("alloc layout align size: {}", layout.align());
                    println!("raw pointer {:p}", raw);
                    NUM_ALLOCS = 0;
                }
                NUM_ALLOCS += 1;

            }
        }
        raw
    }

    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        
        if DRAM_LIMIT_OBJECTS == 0 {
            //all in pmem
            unsafe { allocator_bindings::umf_dealloc(ptr as *mut std::ffi::c_void); }
            #[cfg(debug_assertions)] { ALL_MEM_ALLOCATED.fetch_sub(layout.size(), Ordering::SeqCst); }

            return;
        }
        if DRAM_LIMIT_OBJECTS == 1000 * 1024 * 1024 * 1024 {
            //all in dram
            unsafe { Jemalloc.dealloc(ptr as *mut u8, layout); }
            #[cfg(debug_assertions)] { ALL_MEM_ALLOCATED.fetch_sub(layout.size(), Ordering::SeqCst); }
            return;
        }
    
        // Check the tier of the allocated memory only if not all in pmem or all in dram
        let tier = unsafe {allocator_bindings::check_tier(ptr as *mut std::ffi::c_void)};

        if tier == 0 {
            // DRAM
            unsafe { Jemalloc.dealloc(ptr as *mut u8, layout); }
            DRAM_ALLOCATED_OBJECTS.fetch_sub(layout.size(), Ordering::SeqCst);
            #[cfg(debug_assertions)] { ALL_MEM_ALLOCATED.fetch_sub(layout.size(), Ordering::SeqCst); }
        } else {
            //pmem
            unsafe { allocator_bindings::umf_dealloc(ptr as *mut std::ffi::c_void); }
            #[cfg(debug_assertions)] { ALL_MEM_ALLOCATED.fetch_sub(layout.size(), Ordering::SeqCst); }
        }

        #[cfg(debug_assertions)] {
            unsafe {

                if PRINT_THRESHOLD < NUM_DEALLOCS {
                    let total_alloc = ALL_MEM_ALLOCATED.load(Ordering::SeqCst);
                    let dram_alloc = DRAM_ALLOCATED_OBJECTS.load(Ordering::SeqCst);
                    println!("Allocated: total {} bytes, DRAM {} bytes ({:.2}%), PMEM {} bytes ({:.2}%)",
                        total_alloc,
                        dram_alloc,
                        dram_alloc as f64 / total_alloc as f64 * 100.0,
                        total_alloc - dram_alloc,
                        (total_alloc - dram_alloc) as f64 / total_alloc as f64 * 100.0,
                    );
                    NUM_DEALLOCS = 0;
                    println!("dealloc requested layout size: {}", layout.size());
                    println!("dealloc layout align size: {}", layout.align());
                    println!("raw pointer {:p}", ptr);
                    println!("tier 0 is dram, 1 is pmem: {}", tier);
                }
                NUM_DEALLOCS += 1;
            }
        }
    }
}

impl HybridObjects {
    /// Set the DRAM limit in bytes

    /// Determine whether to allocate from DRAM
    fn should_use_dram(size: usize) -> bool {
        let current = DRAM_ALLOCATED_OBJECTS.load(Ordering::SeqCst); //this was relaxed... but i made it seqcst which might limit performance
        unsafe { current + size <= DRAM_LIMIT_OBJECTS }
    }
}

//needed for allocator api
//seemed to be faster to call the alloc and dealloc for global compared to doing it within the allocator trait.... 
//wierd
unsafe impl Allocator for HybridObjects {
    fn allocate(&self, layout: Layout) -> Result<NonNull<[u8]>, AllocError> {
        unsafe {
            HybridObjects::alloc(self, layout)
                .as_mut()
                .map(|ptr| NonNull::slice_from_raw_parts(NonNull::new_unchecked(ptr), layout.size()))
                .ok_or(AllocError)
        }
    }

    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {
        HybridObjects::dealloc(self, ptr.as_ptr(), layout);
    }
}